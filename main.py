# -*- coding: utf-8 -*-
"""Malaria Disease Prediction from cell Images.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1swLLY4OsaH3ZmX8S_GVo8izI8CYg8eD-

# Convolutional Neural Network
"""

from google.colab import drive
drive.mount('/content/drive')

from google.colab import files
files.upload()

## Installing Kaggle API (If you are using google colab as your IDE)
!pip install -q kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

!chmod 600 ~/.kaggle/kaggle.json


# Downloading the dataset 
!kaggle datasets download -d iarunava/cell-images-for-detecting-malaria

mkdir destination1111

!unzip /content/cell-images-for-detecting-malaria.zip -d destination1111

import shutil
shutil.rmtree('/content/destination1111/cell_images/cell_images')


# # Creating Train / Val / Test folders (One time use)
import os
import numpy as np
import random

root_dir = '/content/destination1111/cell_images'
classes_dir = ['Parasitized', 'Uninfected']

val_ratio = 0.15
test_ratio = 0.05

for cls in classes_dir:
    os.makedirs(root_dir +'train/' + cls)
    os.makedirs(root_dir +'val/' + cls)
    os.makedirs(root_dir +'test/' + cls)


  # Creating partitions of the data after shuffeling
    src = root_dir + "/" +cls   # Folder to copy images from

    allFileNames = os.listdir(src)
    np.random.shuffle(allFileNames)
    train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                          [int(len(allFileNames)* (1 - (val_ratio + test_ratio))), 
                                                           int(len(allFileNames)* (1 - test_ratio))])


    train_FileNames = [src+'/'+ name for name in train_FileNames.tolist()]
    val_FileNames = [src+'/' + name for name in val_FileNames.tolist()]
    test_FileNames = [src+'/' + name for name in test_FileNames.tolist()]

    print('Total images: ', len(allFileNames))
    print('Training: ', len(train_FileNames))
    print('Validation: ', len(val_FileNames))
    print('Testing: ', len(test_FileNames))

  # Copy-pasting images
    for name in train_FileNames:
        shutil.copy(name, root_dir +'train/' + cls)

    for name in val_FileNames:
        shutil.copy(name, root_dir +'val/' + cls)

    for name in test_FileNames:
        shutil.copy(name, root_dir +'test/' + cls)

"""### Importing the libraries"""

import tensorflow as tf
from keras.preprocessing.image import ImageDataGenerator

tf.__version__

train_datagen = ImageDataGenerator(rescale = 1./255,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)
training_set = train_datagen.flow_from_directory('/content/destination1111/cell_imagestrain',
                                                 target_size = (224, 224),
                                                 batch_size = 32,
                                                 class_mode = 'binary')

test_datagen = ImageDataGenerator(rescale = 1./255)
test_set = test_datagen.flow_from_directory('/content/destination1111/cell_imagestest',
                                            target_size = (224, 224),
                                            batch_size = 8,
                                            class_mode = 'binary',
                                            shuffle = False)

val_datagen = ImageDataGenerator(rescale = 1./255)
val_set = val_datagen.flow_from_directory('/content/destination1111/cell_imagesval',
                                            target_size = (224, 224),
                                            batch_size = 32,
                                            class_mode = 'binary')

"""## Part 2 - Building the  CNN Model

### Initialising the Model
"""

import keras,os
import numpy as np

from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, Activation

model = Sequential()


model.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding="same", activation="relu"))
model.add(Conv2D(filters=64,kernel_size=(3,3),padding="same", activation="relu"))
model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))


model.add(Conv2D(filters=128, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=128, kernel_size=(3,3), padding="same", activation="relu"))
model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))


model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))


model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))

model.add(Flatten())
model.add(Dense(units=512,activation="relu"))
model.add(Dense(units=1024,activation="relu"))
model.add(Dense(units=1, activation="sigmoid"))


# Defining the optimizer
opt = tf.keras.optimizers.Adam(learning_rate=0.0001)

# Compiling the model
model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
model.summary()


from keras.callbacks import ModelCheckpoint, EarlyStopping
filepath = '/content/best_Model_weights.h5'
checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')
# early = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=20, mode='auto')


results = model.fit(x=training_set, validation_data= val_set, epochs=15,callbacks=checkpoint)



"""## Part 3 - Plotting Graphical Confusion Matrix"""

import matplotlib.pyplot as plt
import itertools

## Function for plotting the the Confusion matrix without using seaborn package

def plot_confusion_matrix(cm, classes, normalize=True, title='Confusion matrix', cmap=plt.cm.Blues):

    """

    This function prints and plots the confusion matrix.

    Normalization can be applied by setting `normalize=True`.

    """

    plt.figure(figsize=(10,10))


    plt.imshow(cm, interpolation='nearest', cmap=cmap)

    plt.title(title)

    plt.colorbar()


    tick_marks = np.arange(len(classes))

    plt.xticks(tick_marks, classes, rotation=45)

    plt.yticks(tick_marks, classes)


    if normalize:

        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

        cm = np.around(cm, decimals=2)

        cm[np.isnan(cm)] = 0.0

        print("Normalized confusion matrix")

    else:

        print('Confusion matrix, without normalization')

    thresh = cm.max() / 2.

    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):

        plt.text(j, i, cm[i, j],

                 horizontalalignment="center",

                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()

    plt.ylabel('True label')

    plt.xlabel('Predicted label')
    
    
# Evaluating the model on the test set and plotting the Confusion Matrix

from sklearn.metrics import classification_report, confusion_matrix

target_names = []

for key in training_set.class_indices:

    target_names.append(key)

print(target_names)


Y_pred = model.predict_generator(test_set)
y_pred = np.where(Y_pred>0.5, 1, 0)


print('Confusion Matrix')
cm = confusion_matrix(test_set.classes, y_pred)
plot_confusion_matrix(cm, target_names, title='Confusion Matrix')

print(cm)



"""# Part 4-Classification report"""

test_steps_per_epoch = np.math.ceil(test_set.samples / test_set.batch_size)

predictions = model.predict(test_set, steps=test_steps_per_epoch)
# Get most likely class
predicted_classes = np.where(Y_pred>0.5, 1, 0)
class_labels = list(test_set.class_indices.keys())

report = classification_report(test_set.classes, predicted_classes, target_names=class_labels)
print(report)



"""# Part 5-Model accuracy and loss graphs"""

import matplotlib.pyplot as plt
plt.plot(results.history["accuracy"])
plt.plot(results.history['val_accuracy'])
plt.plot(results.history['loss'])
plt.plot(results.history['val_loss'])
plt.title("model accuracy")
plt.ylabel("Accuracy")
plt.xlabel("Epoch")
plt.legend(["Accuracy","Validation Accuracy","loss","Validation Loss"])
plt.show()



"""# Part 6-Saving and Loading the model for prediction"""

from keras.models import model_from_json
from keras.models import load_model

# serialize model to JSON
model_json = model.to_json()
with open("/content/best_Model_weights.json", "w") as json_file:
    json_file.write(model_json)

# load json and create model
json_file = open('/content/best_Model_weights.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(loaded_model_json)

# load weights into new model
loaded_model.load_weights('best_Model_weights.h5')

# evaluate loaded model on test data
loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
score = loaded_model.evaluate(test_set, verbose=0)
print ("%s: %.2f%%" % (loaded_model.metrics_names[1], score[1]*100))

# from keras.models import model_from_json
# from keras.models import load_model

# # serialize model to JSON
# model_json = model.to_json()
# with open("best_Model_weights.json", "w") as json_file:
#     json_file.write(model_json)

# # serialize weights to HDF5
# model.save_weights("best_Model_weights.h5")

# # load json and create model
# json_file = open('best_Model_weights.json', 'r')
# loaded_model_json = json_file.read()
# json_file.close()
# loaded_model = model_from_json(loaded_model_json)

# # load weights into new model
# loaded_model.load_weights("best_Model_weights.h5")
# print("Loaded model from disk")

# loaded_model.save('best_Model_weights.hdf5')
# loaded_model=load_model('best_Model_weights.hdf5')




"""# Part 7- Deploying the model to Predict a single sample."""

# from keras.preprocessing import image
# img = image.load_img("......",target_size=(224,224))
# img = np.asarray(img)

# img = np.expand_dims(img, axis=0)
# loaded_model.predict(img)

import numpy as np
from keras.preprocessing import image
test_image = image.load_img('......', target_size = (224, 224))
test_image = image.img_to_array(test_image)
test_image = np.expand_dims(test_image, axis = 1)
predicted_classes= np.argmax(loaded_model.predict(test_image), axis=1)

result = loaded_model.predict(test_image)
test_set.class_indices
if result[0][0][0] == 1:
  prediction = 'Normal'
elif result[0][1][0] ==1:
  prediction = 'COVID'
else:
  prediction = 'PNEUMONIA'

print(prediction)
print(result)




"""#Section 2 - Using a pre-trained model to train and evaluate the dataset."""

# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D
# from tensorflow.keras import optimizers
from tensorflow.keras import applications
# from keras.models import model_from_json

pre_trained_model = tf.keras.models.Sequential([
  tf.keras.layers.InputLayer((224,224,3)),
  tf.keras.applications.VGG16(include_top=False,weights= 'imagenet', input_shape=(224,224,3)),
  tf.keras.layers.GlobalAveragePooling2D(),
  tf.keras.layers.Dense(512,activation='relu'),
  #tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(1024,activation='relu'),
  # tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(1, activation='sigmoid'),
])

pre_trained_model.summary()
pre_trained_model.compile(optimizer =tf.keras.optimizers.Adam(0.01), loss = 'categorical_crossentropy', metrics = ['accuracy'])

from keras.callbacks import ModelCheckpoint, EarlyStopping
filepath_pre = '/content/best_Model_weights.h5'
checkpoint_pre = ModelCheckpoint(filepath_pre, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')
# early = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=20, mode='auto')


results_pre=pre_trained_model.fit(x = training_set, validation_data = val_set, epochs = 15, callbacks=checkpoint_pre)



"""## Part 3 - Plotting Graphical Confusion Matrix"""


#Evaluating the model on the test set and plotting the Confusion Matrix     
target_names_pre = []

for key in training_set.class_indices:

    target_names_pre.append(key)

print(target_names_pre)

Y_pred_pre = pre_trained_model.predict_generator(test_set)
y_pred_pre = np.argmax(Y_pred_pre, axis=1)


print('Confusion Matrix')
cm_pre = confusion_matrix(test_set.classes, y_pred_pre)
plot_confusion_matrix(cm_pre, target_names_pre, title='Confusion Matrix')



# Classification report
report_pre = classification_report(test_set.classes, y_pred_pre, target_names=target_names_pre)
print(report_pre)


print(np.trace(confusion_matrix(test_set.classes, y_pred_pre)))

from sklearn.metrics import accuracy_score
accuracy_score(test_set.classes, y_pred_pre)
